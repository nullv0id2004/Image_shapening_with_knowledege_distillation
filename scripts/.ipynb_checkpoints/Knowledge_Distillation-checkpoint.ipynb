{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e1c18a37-80a0-4780-ba20-a4df984f6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7fc46328-e18d-4fab-b396-1ccc00aabfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5629b51e-c322-47c6-ae0f-d0d204cd4688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c79cd768-e297-4af7-bc1b-6c72d0d3d474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperResolutionCNN(\n",
       "  (entry): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (res_blocks): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (exit): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upsample): Identity()\n",
       "  (output): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../models')\n",
    "\n",
    "from SuperResolutionCNN import SuperResolutionCNN\n",
    "\n",
    "student = SuperResolutionCNN().to(device)\n",
    "student.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "66f763f5-359b-4be0-9bc7-ad2e787d4f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('C:/Users/manas/image-sharpness/EDSR-PyTorch/src')\n",
    "from model.edsr import make_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d6690601-ee11-441c-ac12-e7435efba6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manas\\AppData\\Local\\Temp\\ipykernel_33560\\1550744298.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teacher.load_state_dict(torch.load('C:/Users/manas/image-sharpness/EDSR-PyTorch/experiment/edsr_sharpness_finetune_x1/model/model_1.pt'))\n"
     ]
    }
   ],
   "source": [
    "args_teacher = argparse.Namespace(\n",
    "    n_resblocks=16,         # number of residual blocks used in training\n",
    "    n_feats=256,            # number of feature maps\n",
    "    res_scale=1.0,          # residual scaling factor\n",
    "    scale=[1],              # upscaling factor (1 for sharpness task)\n",
    "    n_colors=3,             # RGB image (3 channels)\n",
    "    rgb_range=1           # pixel value range (0-255)\n",
    ")\n",
    "teacher = make_model(args_teacher).to(device)\n",
    "teacher.load_state_dict(torch.load('C:/Users/manas/image-sharpness/EDSR-PyTorch/experiment/edsr_sharpness_finetune_x1/model/model_1.pt'))\n",
    "teacher.eval()\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0edb5eb6-edf9-4c28-a04e-a5c86915c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        weights = VGG19_Weights.DEFAULT\n",
    "        vgg = vgg19(weights=weights).features[:36].eval()\n",
    "        for p in vgg.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.vgg = vgg\n",
    "        self.layer_ids = [3, 8, 17, 26, 35]\n",
    "        self.criterion = nn.L1Loss()\n",
    "\n",
    "    def forward(self, sr, hr):\n",
    "        x, y = sr, hr\n",
    "        loss = 0\n",
    "        for i, layer in enumerate(self.vgg):\n",
    "            x = layer(x)\n",
    "            y = layer(y)\n",
    "            if i in self.layer_ids:\n",
    "                loss += self.criterion(x, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0429de7e-e623-41ab-a7af-1275a7162e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDistillationLoss(nn.Module):\n",
    "    def __init__(self, teacher_channels_list):\n",
    "        super().__init__()\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.adapters = nn.ModuleList([\n",
    "            nn.Conv2d(tc, 64, kernel_size=1) for tc in teacher_channels_list\n",
    "        ])\n",
    "\n",
    "    def forward(self, feat_s, feat_t):\n",
    "        loss = 0\n",
    "        for fs, ft, adapter in zip(feat_s, feat_t, self.adapters):\n",
    "            adapted_ft = adapter(ft)\n",
    "            loss += self.criterion(fs, adapted_ft.detach())\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eaa9fcd6-f1d5-48a6-8bb1-e697922d8e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.criterion = nn.L1Loss()\n",
    "\n",
    "    def sobel(self, x):\n",
    "        sobel_x = torch.tensor([[1, 0, -1], [2, 0, -2], [1, 0, -1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "        sobel_y = torch.tensor([[1, 2, 1], [0, 0, 0], [-1, -2, -1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "        sobel_x = sobel_x.repeat(x.size(1), 1, 1, 1).to(x.device)\n",
    "        sobel_y = sobel_y.repeat(x.size(1), 1, 1, 1).to(x.device)\n",
    "        grad_x = F.conv2d(x, sobel_x, padding=1, groups=x.size(1))\n",
    "        grad_y = F.conv2d(x, sobel_y, padding=1, groups=x.size(1))\n",
    "        return torch.sqrt(grad_x ** 2 + grad_y ** 2)\n",
    "\n",
    "    def forward(self, sr, hr):\n",
    "        return self.criterion(self.sobel(sr), self.sobel(hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d674621-1102-46c0-9ccd-115168c78eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_loss_fn = nn.L1Loss()\n",
    "perc_loss_fn = VGGPerceptualLoss().to(device)\n",
    "feat_loss_fn = FeatureDistillationLoss(teacher_channels_list=[256,256,256]).to(device)\n",
    "edge_loss_fn = EdgeLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2faaf50b-603e-4998-9002-a45087daa2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data.sharpness import Sharpness\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.scale = [1]\n",
    "        self.dir_data = 'C:/Users/manas/image-sharpness/EDSR-PyTorch/src/data'  # or wherever your `sharpness/train/HR, LR` dirs are\n",
    "        self.batch_size = 16\n",
    "        self.patch_size = 48\n",
    "        self.n_colors = 3\n",
    "        self.rgb_range = 255\n",
    "        self.test_every = 1000\n",
    "\n",
    "args = Args()\n",
    "\n",
    "train_dataset = Sharpness(args, train=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "val_dataset = Sharpness(args, train=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c839990b-44d5-4700-8963-2fd256c9a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(student.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3f74d524-cfe2-4a5e-9001-38c435243080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 48, 48]) torch.Size([3, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "lr, hr = train_dataset[0]\n",
    "print(lr.shape, hr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "58463bbb-0d39-4adf-8aba-bbce5562daae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/2776 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher output stats: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 1/2776 [00:01<48:53,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False, device='cuda:0') tensor(-0.0885, device='cuda:0') tensor(1.1167, device='cuda:0')\n",
      "Student output stats: tensor(False, device='cuda:0') tensor(-155.9790, device='cuda:0', grad_fn=<MinBackward1>) tensor(92.9339, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "✅ Student output range: -155.97897338867188 92.93385314941406\n",
      "Teacher output stats: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 1/2776 [00:01<1:14:38,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False, device='cuda:0') tensor(-0.0231, device='cuda:0') tensor(1.0376, device='cuda:0')\n",
      "Student output stats: tensor(True, device='cuda:0') tensor(nan, device='cuda:0', grad_fn=<MinBackward1>) tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "❌ NaN in student output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "NaN in student output",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(s_out)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ NaN in student output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN in student output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Student output range:\u001b[39m\u001b[38;5;124m\"\u001b[39m, s_out\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem(), s_out\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     38\u001b[0m s_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(s_out, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: NaN in student output"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "losses = []\n",
    "vgg_mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "vgg_std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "for epoch in range(5):\n",
    "    student.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for lr, hr in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        lr, hr = lr.to(device), hr.to(device)\n",
    "        lr = lr / 255.0\n",
    "        hr = hr / 255.0\n",
    "\n",
    "        if torch.isnan(lr).any():\n",
    "            print(\"❌ NaN detected in Low-Resolution (lr) input!\")\n",
    "            raise ValueError(\"NaN in lr\")\n",
    "\n",
    "        if torch.isnan(hr).any():\n",
    "            print(\"❌ NaN detected in High-Resolution (hr) ground truth!\")\n",
    "            raise ValueError(\"NaN in hr\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_out, t_feats = teacher(lr, return_features=True)\n",
    "            print('Teacher output stats:', torch.isnan(t_out).any(), t_out.min(), t_out.max())\n",
    "        if torch.isnan(t_out).any():\n",
    "            print(\"❌ NaN in teacher output\")\n",
    "            raise ValueError(\"NaN in teacher output\")\n",
    "            \n",
    "\n",
    "        s_out, s_feats = student(lr, return_features=True)\n",
    "        print('Student output stats:', torch.isnan(s_out).any(), s_out.min(), s_out.max())\n",
    "\n",
    "        if torch.isnan(s_out).any():\n",
    "            print(\"❌ NaN in student output\")\n",
    "            raise ValueError(\"NaN in student output\")\n",
    "\n",
    "        print(\"✅ Student output range:\", s_out.min().item(), s_out.max().item())\n",
    "\n",
    "        s_out_norm = (s_out - vgg_mean) / vgg_std\n",
    "        hr_norm = (hr - vgg_mean) / vgg_std\n",
    "\n",
    "        rec_loss = rec_loss_fn(s_out, hr)\n",
    "        perc_loss = perc_loss_fn(s_out_norm, hr_norm)\n",
    "        feat_loss = feat_loss_fn(s_feats, t_feats)\n",
    "        grad_loss = edge_loss_fn(s_out, hr)\n",
    "\n",
    "        total_loss = (\n",
    "            1.0 * rec_loss +\n",
    "            0.1 * perc_loss +\n",
    "            0.1 * feat_loss +\n",
    "            0.1 * grad_loss\n",
    "        )\n",
    "        if torch.isnan(total_loss):\n",
    "            print(\"❌ NaN detected in total_loss!\")\n",
    "            print(f\"rec_loss: {rec_loss.item()}\")\n",
    "            print(f\"perc_loss: {perc_loss.item()}\")\n",
    "            print(f\"feat_loss: {feat_loss.item()}\")\n",
    "            print(f\"grad_loss: {grad_loss.item()}\")\n",
    "            raise ValueError(\"NaN detected — stopping training\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "        losses.append(total_loss.item())\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Plotting loss\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training Loss after Epoch {epoch+1}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save(student.state_dict(), f\"student_epoch{epoch+1}.pth\")\n",
    "\n",
    "    # ---------- Evaluation on Validation Set ----------\n",
    "    student.eval()\n",
    "    total_psnr = 0\n",
    "    total_ssim = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for lr, hr in val_loader:\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            lr = lr / 255.0\n",
    "            hr = hr / 255.0\n",
    "            pred = student(lr)\n",
    "\n",
    "            pred_img = pred[0].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "            hr_img = hr[0].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "            pred_img = np.clip(pred_img, 0, 1)\n",
    "            hr_img = np.clip(hr_img, 0, 1)\n",
    "\n",
    "            total_psnr += psnr(hr_img, pred_img, data_range=1.0)\n",
    "            total_ssim += ssim(hr_img, pred_img, channel_axis=-1, data_range=1.0)\n",
    "            count += 1\n",
    "\n",
    "    avg_psnr = total_psnr / count\n",
    "    avg_ssim = total_ssim / count\n",
    "    psnr_scores.append(avg_psnr)\n",
    "    ssim_scores.append(avg_ssim)\n",
    "\n",
    "    print(f\"Validation PSNR: {avg_psnr:.2f} | SSIM: {avg_ssim:.4f}\")\n",
    "\n",
    "    # Visualize example\n",
    "    grid = make_grid([lr[0].cpu(), pred[0].cpu(), hr[0].cpu()], nrow=3)\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.title(\"LR | Predicted | HR\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfece715-9d3b-4ec2-96d5-2a868acdbb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6008b3-a149-424e-9af6-33e1b45e0791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sharpness)",
   "language": "python",
   "name": "sharpness"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
